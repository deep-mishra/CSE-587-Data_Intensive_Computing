{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code to retrieve the NYTimes articles related to gun violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nytimes.com/2018/03/24/us/gun-rally-urban.html\n",
      "https://www.nytimes.com/2018/03/22/business/youtube-gun-ban.html\n",
      "https://www.nytimes.com/2018/03/24/us/politics/students-lead-huge-rallies-for-gun-control-across-the-us.html\n",
      "https://www.nytimes.com/reuters/2018/03/27/us/politics/27reuters-usa-guns-stevens.html\n",
      "https://www.nytimes.com/aponline/2018/03/26/us/ap-us-gun-restrictions-vermont.html\n",
      "https://www.nytimes.com/2018/03/26/us/california-today-gun-marches.html\n",
      "https://www.nytimes.com/aponline/2018/03/26/business/ap-us-remington-bankruptcy.html\n",
      "https://www.nytimes.com/2018/03/26/podcasts/the-daily/chicago-gun-violence.html\n",
      "https://www.nytimes.com/2018/03/25/us/politics/guns-midterms-republicans.html\n"
     ]
    }
   ],
   "source": [
    "#Script calls the API and retrieve URLs for the articles related to gun violence\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "from lucenequerybuilder import Q\n",
    "\n",
    "urls = list()\n",
    "regexp = re.compile(r'query')\t#regex for urls containg the word query\n",
    "\n",
    "hits = sys.maxsize\n",
    "for i in range(1, 2):\n",
    "    search_query = Q(Q('gun') | Q('gunman'))\n",
    "    # DATE IS IN YYYYMMDD FORMAT\n",
    "    parameters = {\"api-key\": \"API_KEY\",\n",
    "                  \"q\": search_query,\n",
    "                  \"glocations\": \"United States\",\n",
    "                  \"fl\": \"web_url\",\n",
    "                  \"page\": i,\n",
    "                  \"begin_date\": \"20180321\",\n",
    "                  \"end_date\": \"20180329\"}\n",
    "    time.sleep(1)\t# delaying api calls due to api time restrictions\n",
    "    response = requests.get(\"https://api.nytimes.com/svc/search/v2/articlesearch.json\", params=parameters)\n",
    "    data = response.json()\n",
    "    try:\n",
    "        #Getting the number of hits\n",
    "        hits = data['response']['meta']['hits']\n",
    "        # print(hits)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # print(data)\n",
    "        for x in data['response']['docs']:\n",
    "            if regexp.search(x['web_url']):\n",
    "                print(\"regex\")\n",
    "                continue\n",
    "\n",
    "            if '/world/' not in x['web_url'] and '/sports/' not in x['web_url'] and '/video/' not in x['web_url']: #removing unnecessary urls\n",
    "                urls.append(x['web_url'])\n",
    "                print(x['web_url'])\n",
    "    except KeyError:\n",
    "        print(\"error\")\n",
    "        pass\n",
    "    if data['response']['meta']['offset'] > data['response']['meta']['hits']: break # NO more articles available\n",
    "\n",
    "# SAVING TO FILE\n",
    "urls_file = open(\"nytimes_data\\\\sample_url.txt\", \"a+\")\n",
    "for i in urls:\n",
    "    urls_file.write(i + \"\\n\")\n",
    "urls_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files downloaded\n"
     ]
    }
   ],
   "source": [
    "#Script will retrieve the articles for the URLs collected and parses to get the article body \n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "directory = \"nytimes_data\\\\sample_data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "urls = open(\"nytimes_data\\\\sample_url.txt\").read()\n",
    "urls = urls.splitlines()\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    content = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    # Opening the file\n",
    "    file = open('nytimes_data\\\\sample_data\\\\ny_article_' + str(index + 1) + \".txt\", \"w+\")\n",
    "\n",
    "    # Getting the article body and saving to the file\n",
    "    for a in soup.find_all('p', {'class': 'story-body-text story-content'}):\n",
    "        file.write(a.text)\n",
    "    for a in soup.find_all('p', {'class': 'g-body'}):\n",
    "        file.write(a.text)\n",
    "    for a in soup.find_all('p', {'class': 'css-1xyeyil e2kc3sl0'}):\n",
    "        file.write(a.text)\n",
    "    file.close()\n",
    "    \n",
    "print (\"Files downloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
