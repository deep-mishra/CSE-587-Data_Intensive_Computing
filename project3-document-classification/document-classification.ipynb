{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### News articles can be from different categories like sports, business, etc. This project uses spark infrastructure with machine learning to predict the category of articles. The script first trains the model using the training set, test it, and finally evaluate the performance on some unknown article set.\n",
    "\n",
    "\n",
    "#### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINDOWS-5TVLEBR.fios-router.home:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b291610b70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, RegexTokenizer, StopWordsRemover, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Creatingt Spark SQL environment\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|           file_name|                text|category|\n",
      "+--------------------+--------------------+--------+\n",
      "|ny_business_artic...|DAVOS, Switzerlan...|business|\n",
      "|ny_science_articl...|North America was...| science|\n",
      "|ny_business_artic...|Spotify is finall...|business|\n",
      "|ny_movies_article...|Even at a time of...|  movies|\n",
      "|ny_politics_artic...|A Democratic grou...|politics|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total Data (files) : 400\n"
     ]
    }
   ],
   "source": [
    "# Load text files into dataframe\n",
    "\n",
    "dir_names = [\"data/business/*\", \"data/movies/*\", \"data/politics/*\", \"data/science/*\", \"data/sports/*\"]\n",
    "labels = [\"business\", \"movies\", \"politics\", \"science\", \"sports\"]\n",
    "schema = StructType([StructField('file_name', StringType(), True),StructField('text', StringType(), True),StructField('category', StringType(), True)])\n",
    "articles_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "for idx, dir_name in enumerate(dir_names):\n",
    "    articles = spark.sparkContext.wholeTextFiles(dir_name)\n",
    "    articles_data = articles.map(lambda item: (item[0].split('/')[-1],item[1],labels[idx]))\n",
    "    articles_df = articles_df.union(spark.createDataFrame(articles_data, [\"file_name\", \"text\", \"category\"]))\n",
    "\n",
    "articles_df.distinct().show(5)\n",
    "print(\"Total Data (files) :\", articles_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+-----+--------------------+--------------------+\n",
      "|           file_name|                text|category|label|               words|            filtered|\n",
      "+--------------------+--------------------+--------+-----+--------------------+--------------------+\n",
      "|ny_business_artic...|Good Wednesday. H...|business|  1.0|[good, wednesday,...|[good, wednesday,...|\n",
      "|ny_business_artic...|The German carmak...|business|  1.0|[the, german, car...|[german, carmaker...|\n",
      "+--------------------+--------------------+--------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create labels, tokenize and clean data\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Create Labels for the record(containing text from different files of some category)\n",
    "labeledData = StringIndexer(inputCol = \"category\", outputCol = \"label\").fit(articles_df).transform(articles_df)\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizedData = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\").transform(labeledData)\n",
    "\n",
    "# Clean data by removing stopwords\n",
    "cleanData = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").transform(tokenizedData)\n",
    "\n",
    "cleanData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+--------------------+--------------------+\n",
      "|category|label|            filtered|          tfFeatures|            features|\n",
      "+--------+-----+--------------------+--------------------+--------------------+\n",
      "|business|  1.0|[good, wednesday,...|(1000,[1,2,3,5,6,...|(1000,[1,2,3,5,6,...|\n",
      "|business|  1.0|[german, carmaker...|(1000,[3,6,10,23,...|(1000,[3,6,10,23,...|\n",
      "+--------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify features using hashingTF and IDF\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Calculate term frequency of the document\n",
    "hashingTermFreq = HashingTF(inputCol=\"filtered\", outputCol=\"tfFeatures\", numFeatures=1000)\n",
    "tfFeaturedData = hashingTermFreq.transform(cleanData)\n",
    "\n",
    "# Calculate inverse document frequency\n",
    "idf = IDF(inputCol=\"tfFeatures\", outputCol=\"features\")\n",
    "invDocFreqModel = idf.fit(tfFeaturedData)\n",
    "rescaledIdfData = invDocFreqModel.transform(tfFeaturedData)\n",
    "\n",
    "rescaledIdfData.select(\"category\",\"label\",\"filtered\",\"tfFeatures\",\"features\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (files)  283\n",
      "Test Data (files)  117\n"
     ]
    }
   ],
   "source": [
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "\n",
    "train, test = rescaledIdfData.randomSplit([0.7,0.3])\n",
    "print(\"Training Data (files) \",train.count())\n",
    "print(\"Test Data (files) \",test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict partitioned data using Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[1,3,7,8,9,...|  0.0|       0.0|[0.78171980855110...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — For ...|(1000,[2,4,6,13,1...|  0.0|       0.0|[0.68643056261657...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Repr...|(1000,[4,5,7,12,1...|  0.0|       0.0|[0.67677694707072...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[0,3,4,6,8,...|  0.0|       0.0|[0.64902220341435...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Sena...|(1000,[0,1,4,6,9,...|  0.0|       0.0|[0.63167517006392...|\n",
      "|politics|ny_politics_artic...|Update, March 14,...|(1000,[3,4,9,19,2...|  0.0|       0.0|[0.56257592303070...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Phil...|(1000,[1,4,5,7,13...|  0.0|       0.0|[0.50663365922572...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,3,12,19,...|  0.0|       0.0|[0.50308062045447...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[0,1,4,6,7,...|  0.0|       0.0|[0.46408611505266...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — For ...|(1000,[0,1,3,7,8,...|  0.0|       0.0|[0.42791408775884...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.803419\n",
      "Test Error- 0.196581\n"
     ]
    }
   ],
   "source": [
    "# TRAIN and TEST DATA USING RANDOM_FOREST_CLASSIFICATION\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create the RF model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\") \n",
    "# Train model\n",
    "rfModel = rf.fit(train)\n",
    "\n",
    "# Predict test data\n",
    "rfPredictions = rfModel.transform(test)\n",
    " \n",
    "print(\"TEST RESULT:\")\n",
    "# Show the result of prediction with the probability\n",
    "rfPredictions.filter(rfPredictions['prediction'] == 0)\\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\",ascending=False).show(10)\n",
    "\n",
    "# Evaluate the prediction result\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(rfPredictions)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING RANDOM FOREST CLASSIFICATION MODEL ON UNKNOWN DATA (not test data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new files loaded (only for testing) : 100\n",
      "\n",
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|LIMA, Peru — As P...|(1000,[1,3,5,11,1...|  0.0|       0.0|[0.78805006569284...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — With...|(1000,[1,4,5,7,9,...|  0.0|       0.0|[0.66155175880286...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[0,1,4,5,6,...|  0.0|       0.0|[0.61061653168505...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,6,7,10,1...|  0.0|       0.0|[0.59713316590150...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[3,4,5,6,7,...|  0.0|       0.0|[0.53992632368327...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — In S...|(1000,[0,1,4,6,7,...|  0.0|       0.0|[0.52898439445918...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — In P...|(1000,[1,6,13,15,...|  0.0|       0.0|[0.50442616494613...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[1,5,6,13,1...|  0.0|       0.0|[0.50072419966985...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Hous...|(1000,[0,3,4,5,6,...|  0.0|       0.0|[0.48152284577039...|\n",
      "|politics|ny_politics_artic...|From the administ...|(1000,[1,3,8,13,1...|  0.0|       0.0|[0.46340764373294...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.77\n",
      "Test Error- 0.23\n"
     ]
    }
   ],
   "source": [
    "# LOAD NEW FILES (TEST FILES)\n",
    "\n",
    "new_dir_names = [\"new_data/business/*\", \"new_data/movies/*\", \"new_data/politics/*\", \"new_data/science/*\", \"new_data/sports/*\"]\n",
    "labels = [\"business\", \"movies\", \"politics\", \"science\", \"sports\"]\n",
    "schema = StructType([StructField('file_name', StringType(), True),StructField('text', StringType(), True),StructField('category', StringType(), True)])\n",
    "new_articles_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "for idx, new_dir_name in enumerate(new_dir_names):\n",
    "    new_articles = spark.sparkContext.wholeTextFiles(new_dir_name)\n",
    "    new_articles_data = new_articles.map(lambda item: (item[0].split('/')[-1],item[1],labels[idx]))\n",
    "    new_articles_df = new_articles_df.union(spark.createDataFrame(new_articles_data, [\"file_name\", \"text\", \"category\"]))\n",
    "\n",
    "print(\"Total new files loaded (only for testing) :\", new_articles_df.count())\n",
    "print()\n",
    "\n",
    "# label, Clean, and Extract Features\n",
    "labeledTestData = StringIndexer(inputCol = \"category\", outputCol = \"label\").fit(articles_df).transform(new_articles_df)\n",
    "tokenizedTestData = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\").transform(labeledTestData)\n",
    "clnTestData = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").transform(tokenizedTestData)\n",
    "featuredTestData = HashingTF(inputCol=\"filtered\", outputCol=\"tfFeatures\", numFeatures=1000).transform(clnTestData)\n",
    "rescaledTestData = IDF(inputCol=\"tfFeatures\", outputCol=\"features\").fit(tfFeaturedData).transform(featuredTestData)\n",
    "\n",
    "# Predict the labels\n",
    "testPrediction = rfModel.transform(rescaledTestData)\n",
    " \n",
    "print(\"TEST RESULT:\")\n",
    "# Show the result of prediction with the probability\n",
    "testPrediction.filter(testPrediction['prediction'] == 0)\\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\",ascending=False).show(10)\n",
    "\n",
    "# Evaluate accuracy\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(testPrediction)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict partitioned data using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|• President Trump...|(1000,[0,1,3,4,7,...|  0.0|       0.0|[0.99985646975422...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,3,12,19,...|  0.0|       0.0|[0.98788680063328...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[3,4,6,7,10...|  0.0|       0.0|[0.94510932410313...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[1,3,7,8,9,...|  0.0|       0.0|[0.92211608608754...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Sena...|(1000,[0,1,4,6,9,...|  0.0|       0.0|[0.88301017166012...|\n",
      "|politics|ny_politics_artic...|Update, March 14,...|(1000,[3,4,9,19,2...|  0.0|       0.0|[0.88234135756463...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Repr...|(1000,[4,5,7,12,1...|  0.0|       0.0|[0.87936989155866...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Phil...|(1000,[1,4,5,7,13...|  0.0|       0.0|[0.86254138301729...|\n",
      "|politics|ny_politics_artic...|A Democratic grou...|(1000,[0,3,4,6,8,...|  0.0|       0.0|[0.83040815300005...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,3,8,21,2...|  0.0|       0.0|[0.81161867264165...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.931624\n",
      "Test Error- 0.0683761\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST DATA USING LOGISTIC_REGRESSION\n",
    "\n",
    "# Apply Logistic Regression to create the model, train, and predict on test data.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# Train model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Predict Testdata\n",
    "lrPredictions = lrModel.transform(test)\n",
    "\n",
    "print(\"TEST RESULT:\")\n",
    "# Display prediction result and its probability\n",
    "lrPredictions.filter(lrPredictions['prediction'] == 0) \\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\",ascending=False).show(10)\n",
    "\n",
    "# Evaluate the prediction result\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(lrPredictions)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING LOGISTIC REGRESSION MODEL ON UNKNOWN DATA (not test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new files loaded (only for testing) : 100\n",
      "\n",
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,6,7,10,1...|  0.0|       0.0|[0.98255504585349...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[3,4,5,6,7,...|  0.0|       0.0|[0.97165103767754...|\n",
      "|politics|ny_politics_artic...|BLOOMINGTON, Ind....|(1000,[0,1,2,3,4,...|  0.0|       0.0|[0.95672635635010...|\n",
      "|politics|ny_politics_artic...|LIMA, Peru — As P...|(1000,[1,3,5,11,1...|  0.0|       0.0|[0.88470932343682...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — With...|(1000,[1,4,5,7,9,...|  0.0|       0.0|[0.87240393340621...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[2,3,7,9,10...|  0.0|       0.0|[0.86566492454170...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[1,5,6,13,1...|  0.0|       0.0|[0.84419043383731...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — In P...|(1000,[1,6,13,15,...|  0.0|       0.0|[0.80078990444453...|\n",
      "| science|ny_science_articl...|Seven and a half ...|(1000,[0,1,2,3,4,...|  3.0|       0.0|[0.79073039141751...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — In S...|(1000,[0,1,4,6,7,...|  0.0|       0.0|[0.74970672241838...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.93\n",
      "Test Error- 0.07\n"
     ]
    }
   ],
   "source": [
    "# LOAD NEW FILES (TEST FILES)\n",
    "\n",
    "new_dir_names = [\"new_data/business/*\", \"new_data/movies/*\", \"new_data/politics/*\", \"new_data/science/*\", \"new_data/sports/*\"]\n",
    "labels = [\"business\", \"movies\", \"politics\", \"science\", \"sports\"]\n",
    "schema = StructType([StructField('file_name', StringType(), True),StructField('text', StringType(), True),StructField('category', StringType(), True)])\n",
    "new_articles_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "for idx, new_dir_name in enumerate(new_dir_names):\n",
    "    new_articles = spark.sparkContext.wholeTextFiles(new_dir_name)\n",
    "    new_articles_data = new_articles.map(lambda item: (item[0].split('/')[-1],item[1],labels[idx]))\n",
    "    new_articles_df = new_articles_df.union(spark.createDataFrame(new_articles_data, [\"file_name\", \"text\", \"category\"]))\n",
    "\n",
    "print(\"Total new files loaded (only for testing) :\", new_articles_df.count())\n",
    "print()\n",
    "\n",
    "# label, Clean, and Extract Features\n",
    "labeledTestData2 = StringIndexer(inputCol = \"category\", outputCol = \"label\").fit(articles_df).transform(new_articles_df)\n",
    "tokenizedTestData2 = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\").transform(labeledTestData2)\n",
    "clnTestData2 = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").transform(tokenizedTestData2)\n",
    "featuredTestData2 = HashingTF(inputCol=\"filtered\", outputCol=\"tfFeatures\", numFeatures=1000).transform(clnTestData2)\n",
    "rescaledTestData2 = IDF(inputCol=\"tfFeatures\", outputCol=\"features\").fit(tfFeaturedData).transform(featuredTestData2)\n",
    "\n",
    "# Predict labels\n",
    "testPrediction = lrModel.transform(rescaledTestData2)\n",
    " \n",
    "print(\"TEST RESULT:\")\n",
    "# Show the result of prediction with the probability\n",
    "testPrediction.filter(testPrediction['prediction'] == 0)\\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\",ascending=False).show(10)\n",
    "\n",
    "# Evaluate accuracy\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(testPrediction)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeate the above process using pipeline (with Random Forest Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|WASHINGTON — When...|(1000,[1,9,13,16,...|  0.0|       0.0|[0.66803400508571...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[0,3,4,6,8,...|  0.0|       0.0|[0.65384135411594...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — In a...|(1000,[0,1,3,7,13...|  0.0|       0.0|[0.64296410178251...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[0,1,3,5,6,...|  0.0|       0.0|[0.62832395566357...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,3,12,19,...|  0.0|       0.0|[0.59525478052635...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Phil...|(1000,[1,4,5,7,13...|  0.0|       0.0|[0.56885299605180...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,7,13,15,...|  0.0|       0.0|[0.54163111139272...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[0,3,6,11,1...|  0.0|       0.0|[0.53319783942449...|\n",
      "|politics|ny_politics_artic...|In March, The New...|(1000,[0,3,4,7,9,...|  0.0|       0.0|[0.51642012017604...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Geor...|(1000,[3,4,6,13,1...|  0.0|       0.0|[0.50137259863459...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.817391\n",
      "Test Error- 0.182609\n"
     ]
    }
   ],
   "source": [
    "# APPLICATION OF PIPELINE (with Random Forest Classification)\n",
    "\n",
    "# Define all the transformations\n",
    "labelIndexer = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "regTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"itfFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"itfFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "# Pipeline all the transformations\n",
    "pipeline = Pipeline(stages=[labelIndexer, regTokenizer, stopWordsRemover, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(articles_df)\n",
    "data = pipelineModel.transform(articles_df)\n",
    "\n",
    "# Split the data in training and test\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create Logistic Regression model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "# Train the model\n",
    "rfModel = rf.fit(train)\n",
    "# Predict the test data\n",
    "rfPredictions = rfModel.transform(test)\n",
    "print(\"TEST RESULT:\")\n",
    "# Display prediction result and prediction probability\n",
    "rfPredictions.filter(rfPredictions['prediction'] == 0) \\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\", ascending=False).show(10)\n",
    "    \n",
    "# Evaluate the performance of the model\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(rfPredictions)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeate the above process using pipeline (with Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULT:\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|category|           file_name|                text|            features|label|prediction|         probability|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "|politics|ny_politics_artic...|WASHINGTON — Fift...|(1000,[0,1,3,4,6,...|  0.0|       0.0|[0.94852034162383...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[1,3,7,8,9,...|  0.0|       0.0|[0.94761729681854...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — From...|(1000,[0,3,4,5,6,...|  0.0|       0.0|[0.94406212285865...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Few ...|(1000,[0,1,3,4,6,...|  0.0|       0.0|[0.93880856642434...|\n",
      "|politics|ny_politics_artic...|Update, March 14,...|(1000,[3,4,9,19,2...|  0.0|       0.0|[0.93855943813865...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Pres...|(1000,[0,2,3,4,6,...|  0.0|       0.0|[0.91251806216755...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — Atto...|(1000,[4,16,19,20...|  0.0|       0.0|[0.90015246474117...|\n",
      "|politics|ny_politics_artic...|WASHINGTON —  Sen...|(1000,[3,4,6,18,2...|  0.0|       0.0|[0.87292947516727...|\n",
      "|politics|ny_politics_artic...|WASHINGTON — The ...|(1000,[1,7,13,15,...|  0.0|       0.0|[0.85953307364421...|\n",
      "|politics|ny_politics_artic...|PHOENIX — Represe...|(1000,[0,1,2,4,5,...|  0.0|       0.0|[0.85833329182165...|\n",
      "+--------+--------------------+--------------------+--------------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy- 0.958333\n",
      "Test Error- 0.0416667\n"
     ]
    }
   ],
   "source": [
    "# APPLICATION OF PIPELINE (with Logistic Regression)\n",
    "\n",
    "# Define all the transformations\n",
    "labelIndexer = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "regTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"itfFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"itfFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "# Pipeline all the transformations\n",
    "pipeline = Pipeline(stages=[labelIndexer, regTokenizer, stopWordsRemover, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(articles_df)\n",
    "data = pipelineModel.transform(articles_df)\n",
    "\n",
    "# Split the data in training and test\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create logistic regression model\n",
    "# Create model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "# Train model\n",
    "lrModel = lr.fit(train)\n",
    "# Predict Testdata\n",
    "lrPredictions = lrModel.transform(test)\n",
    "print(\"TEST RESULT:\")\n",
    "# Display prediction result and prediction probability\n",
    "lrPredictions.filter(lrPredictions['prediction'] == 0) \\\n",
    "    .select(\"category\",\"file_name\",\"text\",\"features\",\"label\",\"prediction\",\"probability\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(10)\n",
    "    \n",
    "# Evaluate the performance of the model\n",
    "mulClassEvl = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = mulClassEvl.evaluate(lrPredictions)\n",
    "print(\"Accuracy- %g\" % accuracy)\n",
    "print(\"Test Error- %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "https://creativedata.atlassian.net/wiki/spaces/SAP/pages/83237142/Pyspark+-+Tutorial+based+on+Titanic+Dataset<br>\n",
    "https://www.tutorialkart.com/apache-spark/spark-mllib-tf-idf/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
